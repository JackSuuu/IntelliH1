# IntelliH1: Cognitive Humanoid Framework

> **Building reliable robot intelligence from first principles**

An intelligent humanoid navigation system combining LLM-based planning (Groq API) with the official Unitree RL walking controller for the Unitree H1 robot.


![Status](https://img.shields.io/badge/status-stable-brightgreen)
![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20Linux-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![MuJoCo 3.3+](https://img.shields.io/badge/MuJoCo-3.3+-green.svg)](https://mujoco.org/)
[![Unitree RL](https://img.shields.io/badge/Unitree-RL_Gym-blue)](https://github.com/unitreerobotics/unitree_rl_gym)

## Demo

https://github.com/user-attachments/assets/b1d41aa8-13d5-47da-9389-0d6525d0fc74

---

## Table of Contents

- [Features](#features)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Technical Architecture](#technical-architecture)
- [Control Modes](#control-modes)
- [Known Issues](#known-issues)
- [Development Notes](#development-notes)
- [References](#references)
- [License](#license)

---

## âœ¨ Features

### ğŸ¯ Core Capabilities

1. **â­ Official Unitree RL Walking Controller** (âœ… Stable)
   - Pre-trained RL policy from [unitree_rl_gym](https://github.com/unitreerobotics/unitree_rl_gym)
   - 30+ seconds continuous stable walking
   - Supports H1, H1_2, and G1 robots
   - **Configurable walking speed: 0.5-3.0 m/s** (recommended: 1.0-1.5 m/s)
   - PD tracking with optimized gains
   - Adaptive velocity control based on heading error

2. **ğŸ¤– LLM-Driven Navigation** (âœ… Production Ready)
   - Natural language command understanding (Groq API + openai/gpt-oss-120b)
   - Automatic target parsing ("walk to the kitchen" â†’ target coordinates)
   - **Real-time navigation with intelligent path following**
   - **Goal-based stopping with 1.2m tolerance**
   - Scene landmarks: Kitchen, Bedroom, Living Room
   - Dynamic waypoint progression tracking

3. **ğŸ—ºï¸ A* Path Planning** (âœ… Stable)
   - Obstacle-aware global path planning
   - Dynamic occupancy grid mapping
   - Waypoint generation with ~0.3m resolution
   - Real-time path updates every 0.5s
   - Collision avoidance with static obstacles

4. **ğŸ“¡ C++ Radar Perception** (âœ… Fully Integrated)
   - **Real-time LIDAR simulation (360Â° coverage, 10m range)**
   - **C++ optimized point cloud processing (pybind11)**
   - Noise filtering and obstacle detection
   - Occupancy grid generation for path planning
   - ~360 samples per scan with sub-10ms processing time

5. **ğŸ—ºï¸ Enhanced Environment Scene** (âœ… Available)
   - Kitchen area with counter and cabinet (5.0, 3.0)
   - Bedroom with bed and nightstand (-3.0, 6.0)
   - Living room with couch and coffee table (0.0, -4.0)
   - Manipulable objects (cubes, spheres)

### ğŸ› ï¸ Tech Stack

- **Physics Engine**: MuJoCo 3.3+
- **Robot Model**: Unitree H1 (URDF/MJCF)
- **Python Version**: 3.10+
- **Core Dependencies**: NumPy, MuJoCo Python bindings
- **AI Integration**: Groq API (Llama 3.3) + LangChain
- **Perception**: OpenCV + C++ optimized point cloud processing
- **Dynamics**: Pinocchio for rigid body dynamics
- **Optimization**: OSQP for quadratic programming

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository with submodules
git clone --recursive https://github.com/JackSuuu/IntelliH1.git
cd IntelliH1

# Create conda environment
conda create -n intellih1 python=3.10
conda activate intellih1

# Install dependencies
pip install mujoco numpy groq python-dotenv
chmod +x scripts/install_deps.sh
./scripts/install_deps.sh

# Setup Groq API for LLM planning
echo "GROQ_API_KEY=your_key_here" > .env

# Compile C++ perception module (optional but recommended)
cd src/perception/cpp
g++ -O3 -Wall -shared -std=c++11 -fPIC \
    $(python3 -m pybind11 --includes) \
    bindings.cpp point_cloud_processor.cpp \
    -o perception_cpp$(python3-config --extension-suffix) \
    $(python3-config --ldflags) \
    -L$(python3-config --prefix)/lib -lpython3.10
cd ../../..
```

### Basic Usage

```bash
# Make demo script executable
chmod +x demo.sh

# ğŸ¯ Quick navigation examples
./demo.sh kitchen                    # Go to kitchen
./demo.sh bedroom                    # Go to bedroom
./demo.sh "walk to living room"      # Natural language

# âš¡ Advanced usage
./demo.sh --speed 1.2 bedroom        # Fast navigation
./demo.sh --speed 0.8 kitchen        # Slower, more stable

# ğŸ“‹ View all options
./demo.sh --help
```

### Available Locations

| Location | Coordinates | Description |
|----------|-------------|-------------|
| **Kitchen** | (5.0, 3.0) | Kitchen area with counter and cabinet |
| **Bedroom** | (-3.0, 6.0) | Bedroom with bed and nightstand |
| **Living Room** | (0.0, -4.0) | Living room with couch and coffee table |

### Expected Behavior

- **LLM Navigation**: Robot understands natural language commands and plans path to destination
- **C++ Perception**: Real-time LIDAR processing (360 samples) with C++ optimized point cloud filtering
- **Stable Walking**: Robot walks at configurable speed (0.5-3.0 m/s, recommended 1.0-1.5 m/s)
- **Intelligent Control**: Adaptive velocity based on heading error (slows when turning, speeds up when aligned)
- **Goal Detection**: Automatic stopping within 1.2m of destination
- **Path Completion**: Follows 9-waypoint A* path and stops at destination (~6-8m path in 8-12 seconds at 1.0 m/s)

---

## ğŸ“ Project Structure

```
IntelliH1/
â”œâ”€â”€ demo.sh                          # Quick demo launcher
â”œâ”€â”€ COGNITIVE_ARCHITECTURE.md        # Cognitive system documentation
â”œâ”€â”€ reflection.md                    # Development reflections
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ setup.py                         # Package setup
â”‚
â”œâ”€â”€ extern/                          # External dependencies
â”‚   â””â”€â”€ unitree_rl_gym/              # Official Unitree RL Gym
â”‚       â”œâ”€â”€ legged_gym/              # RL training environment
â”‚       â”‚   â”œâ”€â”€ envs/                # Robot environments (H1, H1_2, G1)
â”‚       â”‚   â”œâ”€â”€ scripts/             # Training scripts
â”‚       â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â”œâ”€â”€ deploy/                  # Deployment modules
â”‚       â”‚   â”œâ”€â”€ deploy_mujoco/       # MuJoCo deployment
â”‚       â”‚   â”œâ”€â”€ deploy_real/         # Real robot deployment
â”‚       â”‚   â””â”€â”€ pre_train/           # Pre-trained policies
â”‚       â””â”€â”€ resources/               # Robot URDF/MJCF models
â”‚           â””â”€â”€ robots/
â”‚               â”œâ”€â”€ h1/              # H1 robot model
â”‚               â”œâ”€â”€ h1_2/            # H1_2 robot model
â”‚               â””â”€â”€ g1_description/  # G1 robot model
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ install_deps.sh              # Dependency installer
â”‚
â”œâ”€â”€ src/                             # Main source code
â”‚   â”œâ”€â”€ config.py                    # Global configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ control/                     # Control system (cognitive architecture)
â”‚   â”‚   â”œâ”€â”€ cognitive_controller.py  # ğŸ§  Top-level cognitive controller
â”‚   â”‚   â”‚                            #    - LLM integration
â”‚   â”‚   â”‚                            #    - Goal detection (1.2m tolerance)
â”‚   â”‚   â”‚                            #    - Path planning coordination
â”‚   â”‚   â””â”€â”€ unitree_rl_controller.py # ğŸš¶ Unitree RL motion controller
â”‚   â”‚                                #    - Walking controller with RL policy
â”‚   â”‚                                #    - Adaptive velocity (heading-based)
â”‚   â”‚                                #    - Navigation control (max 0.8 rad/s)
â”‚   â”‚                                #    - Configurable speed (0.5-3.0 m/s)
â”‚   â”‚
â”‚   â”œâ”€â”€ robot/
â”‚   â”‚   â””â”€â”€ humanoid.py              # Unitree H1 robot interface
â”‚   â”‚
â”‚   â”œâ”€â”€ perception/                  # Perception system
â”‚   â”‚   â”œâ”€â”€ multimodal_perception.py # Main perception pipeline
â”‚   â”‚   â”œâ”€â”€ vision.py                # RGB-D processing
â”‚   â”‚   â”œâ”€â”€ path_planner.py          # ğŸ—ºï¸ A* path planning (0.3m resolution)
â”‚   â”‚   â””â”€â”€ cpp/                     # ğŸ“¡ C++ optimized modules
â”‚   â”‚       â”œâ”€â”€ bindings.cpp         #    - pybind11 bindings
â”‚   â”‚       â”œâ”€â”€ point_cloud_processor.cpp  # LIDAR processing
â”‚   â”‚       â””â”€â”€ point_cloud_processor.h    # (360Â° Ã— 10m range)
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                         # ğŸ§  LLM integration
â”‚   â”‚   â”œâ”€â”€ groq.py                  # Groq API wrapper
â”‚   â”‚   â”œâ”€â”€ planner.py               # Task planner
â”‚   â”‚   â”œâ”€â”€ navigation_planner.py    # Navigation command parser
â”‚   â”‚   â”œâ”€â”€ rag_system.py            # RAG pipeline
â”‚   â”‚   â””â”€â”€ physics_kb.py            # Physics knowledge base
â”‚   â”‚
â”‚   â”œâ”€â”€ simulation/
â”‚   â”‚   â””â”€â”€ environment.py           # MuJoCo simulation wrapper
â”‚   â”‚
â”‚   â””â”€â”€ test/                        # Test scripts
â”‚       â”œâ”€â”€ test_h1_scene.py         # H1 scene tests
â”‚       â””â”€â”€ test_llm_navigation.py   # ğŸ¯ LLM navigation demo (main entry)
â”‚
â””â”€â”€ .env                             # API keys (create this)
```

### Key Components

| Component | Purpose | Status |
|-----------|---------|--------|
| **cognitive_controller.py** | Top-level cognitive control integrating LLM â†’ Perception â†’ Planning â†’ Motion | âœ… Production |
| **unitree_rl_controller.py** | Unitree RL walking with adaptive velocity and heading correction | âœ… Stable |
| **path_planner.py** | A* global path planning with dynamic obstacle avoidance | âœ… Stable |
| **perception_cpp** | C++ optimized LIDAR processing (360 samples, <10ms) | âœ… Integrated |
| **navigation_planner.py** | LLM-based command parsing and target extraction | âœ… Production |
| **test_llm_navigation.py** | Main demo script for LLM-driven navigation | âœ… Ready |

---

## ğŸ—ï¸ Technical Architecture

### Control Flow

```text
User Command (demo.sh)
    â†“
test_llm_navigation.py (Main Entry Point)
    â†“
CognitiveController (Top-Level)
    â†“
[4-Layer Cognitive Architecture]
    â”œâ”€â”€ ğŸ§  LLM Planning (Groq API + Llama 3.3)
    â”œâ”€â”€ ğŸ‘ï¸ C++ Perception (LIDAR â†’ Point Cloud)
    â”œâ”€â”€ ğŸ—ºï¸ A* Path Planning (Waypoint Generation)
    â””â”€â”€ ğŸš¶ Unitree RL Motion (Walking Control)
    â†“
MuJoCo Physics Engine
    â†“
Viewer (Visualization)
```

### System Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ§  LLM Planning Layer                      â”‚
â”‚            Groq API + Llama 3.3 70B                     â”‚
â”‚   Natural Language â†’ Target Position & Speed           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸ‘ï¸ C++ Perception Layer                      â”‚
â”‚   LIDAR (360Â° Ã— 10m) + Point Cloud Processing          â”‚
â”‚   C++ pybind11 â†’ Noise Filtering â†’ Occupancy Map       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸ—ºï¸ A* Path Planning Layer                    â”‚
â”‚        Grid-based A* + Dynamic Obstacles               â”‚
â”‚   Start + Goal + Map â†’ 9-Waypoint Sequence             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ğŸš¶ Unitree RL Motion Control                  â”‚
â”‚   Heading Error Correction + Velocity Scaling          â”‚
â”‚   Waypoints â†’ (vx, Ï‰) Commands â†’ RL Policy â†’ Torques   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ® MuJoCo Physics Engine                   â”‚
â”‚      Unitree H1 (19 DOF) + Enhanced Scene              â”‚
â”‚      Kitchen | Bedroom | Living Room + Obstacles       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Control Layer Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Simulation** | MuJoCo 3.3+ | Physics simulation |
| **Dynamics** | Pinocchio 2.6+ | Rigid body dynamics |
| **Optimization** | OSQP 0.6+ | Quadratic programming |
| **Perception** | OpenCV + C++ | Image & point cloud processing |
| **AI** | Groq (Llama 3.3) + LangChain | Task planning & RAG |

---

## ğŸ® Control Modes

### 1. Standing Balance Control

```bash
./demo.sh
```

**Implementation Principle**:

- **PD Control**: $\tau = K_p(q_{target} - q) - K_d \dot{q}$
- **Gravity Compensation**: $\tau_{total} = \tau_{PD} + \tau_{gravity}$
- **Balance Strategies**:

```python
ankle_comp = -130 * pos_x - 22 * vel_x   # Ankle strategy
hip_comp = -55 * pos_x - 9 * vel_x       # Hip strategy
torso_comp = -250 * pos_x - 30 * vel_x   # Torso strategy
```

**Controller Parameters**:

| Joint Group | Kp Gain | Kd Gain | Max Torque |
|-------------|---------|---------|-----------|
| Leg Hips | 300-500 | 25-42 | 220-360 Nm |
| Knees | 400-500 | 33-42 | 360 Nm |
| Ankles | 200 | 17 | 75 Nm |
| Torso | 450 | 38 | 200 Nm |
| Arms | 80-120 | 7-10 | 80 Nm |

### 2. Walking Control (Experimental)

```bash
./demo.sh --walk --walk-speed 0.15
```

**Implemented Methods**:

#### Method A: Sine Gait

- Uses sine waves to generate periodic leg motion
- Step frequency: 0.8 Hz
- Step length: 0.05-0.10 m
- Issue: Dynamically unstable, falls within 2-4 seconds

#### Method B: Quasi-Static Walking

- State machine: standing â†’ shift_left â†’ step_right â†’ shift_right â†’ step_left
- Each state lasts 0.5-0.6 seconds
- Issue: Center of mass transfer causes imbalance

#### Method C: ZMP Control (In Development)

```python
# ZMP calculation formula
zmp_x = com_x - (com_z / g) * com_vx
zmp_y = com_y - (com_z / g) * com_vy

# Stability criterion: ZMP must be within support polygon
is_stable = zmp in support_polygon
```

### 3. LLM-Driven Navigation with A* Path Planning

```bash
# Navigate with natural language commands
./demo.sh kitchen                    # Go to kitchen
./demo.sh bedroom                    # Go to bedroom  
./demo.sh "walk to living room"      # Natural language

# Configure speed (0.5-3.0 m/s, recommended: 1.0-1.5 m/s)
./demo.sh --speed 1.0 bedroom        # Balanced speed
./demo.sh --speed 1.5 kitchen        # Fast navigation
./demo.sh --speed 0.8 living_room    # Slower, more stable
```

**Navigation Features**:

- **4-Layer Cognitive Architecture**: LLM â†’ C++ Perception â†’ A* Planning â†’ RL Motion
- **Intelligent Velocity Control**: Slows down when turning (60Â°+ error â†’ 0.1Ã— speed), speeds up when aligned
- **Goal Detection**: Automatically stops within 1.2m of destination
- **Waypoint Progression**: Tracks progress through 9-waypoint path with real-time feedback
- **Heading Correction**: Proportional control with 0.8 rad/s max turn rate

**Preset Locations**:

- Kitchen: (5.0, 3.0)
- Bedroom: (-3.0, 6.0)
- Living Room: (0.0, -4.0)

---

## âš ï¸ Known Issues

### âœ… Recently Fixed

1. **Navigation Speed Configuration** (Fixed âœ…)
   - **Issue**: Command-line `--speed` parameter was ignored
   - **Solution**: Propagated `max_speed` through controller hierarchy (CognitiveController â†’ UnitreeRLWalkingController â†’ UnitreeRLController)
   - **Status**: Speed now fully configurable from 0.5-3.0 m/s

2. **Robot Not Stopping at Destination** (Fixed âœ…)
   - **Issue**: Robot reached goal area but continued spinning
   - **Solution**:
     - Implemented goal-based stopping (1.2m tolerance around final destination)
     - Added distance-to-goal checking independent of waypoint progression
     - Velocity and target cleared upon goal detection
   - **Status**: Robot now reliably stops at destination

3. **Heading Error Causing Drift** (Fixed âœ…)
   - **Issue**: Robot walking in wrong direction with constant omega=0.5 rad/s
   - **Solution**:
     - Adaptive velocity scaling based on heading error (60Â°+ â†’ 10% speed, 30Â°+ â†’ 30% speed)
     - Increased turn rate to 0.8 rad/s for faster heading correction
     - Added NAV DEBUG logging every 100 cycles
   - **Status**: Robot now corrects heading before moving forward

### ğŸ”´ Active Issues

1. **Platform Dependencies**
   - macOS requires `mjpython` (auto-detected in demo.sh)
   - Linux and macOS viewer behavior may differ
   - **Workaround**: Auto-detection in demo.sh

2. **C++ Perception Module Compilation**
   - Requires manual compilation with pybind11
   - Platform-specific library paths may need adjustment
   - **Workaround**: Fallback to Python LIDAR simulation if C++ module unavailable

### ğŸŸ¡ Minor Issues

- Missing comprehensive unit tests
- Navigation tolerance (1.2m) may be too large for precise positioning
- No dynamic obstacle avoidance (only static obstacle map)
- Limited error recovery mechanisms
- Performance metrics not logged systematically

---

## ğŸ“ Development Notes

### Why is Bipedal Walking So Hard?

This project demonstrates the core challenges of bipedal robot control:

#### 1. **Under-Actuated System**

- Robot has 6 DOF floating base (position + orientation)
- Can only control indirectly through joint torques
- Must rely on ground contact forces for motion

#### 2. **Dynamic Instability**

- High center of mass, small support surface
- Any small disturbance accumulates rapidly
- Requires continuous active control to maintain balance

#### 3. **Contact Switching**

- Walking involves transitions: double support â†’ single support â†’ double support
- Each switch is a potential instability point
- Requires precise timing control

#### 4. **Parameter Sensitivity**

```python
# These parameters changing by 10% can cause failure
ankle_gain = 130.0  # Â±13 affects stability
knee_stance = 0.25   # Â±0.025 affects CoM height
step_frequency = 0.8 # Â±0.08 changes gait rhythm
```

### Experimental Records

#### Experiment 1: Basic PD Standing

- **Date**: 2025-11-10
- **Parameters**: Kp=[300,250,400,500,200], Kd=Kp/12
- **Result**: Falls after 6 seconds
- **Issue**: Forward drift 0.01mâ†’0.40m

#### Experiment 2: Adaptive Gains

- **Modification**: `gain = base_gain * (1 + 5*|error|)`
- **Result**: No significant improvement, still falls after 6 seconds
- **Conclusion**: Simply increasing gains insufficient, need different control strategy

#### Experiment 3: Quasi-Static Walking

- **Strategy**: State machine controls center of mass transfer
- **Result**: Immediate imbalance after warmup
- **Cause**: hip_roll compensation conflicts with PD control

#### Experiment 4: Pure PD Walking (Remove Balance Compensation)

- **Modification**: No compensation during walking
- **Result**: Stable for first 6 seconds, falls when starting leg swing
- **Conclusion**: Gait parameters still need significant optimization

### Successful Baseline

The following configuration achieves **stable standing for 4-6 seconds**:

```python
# src/control/pd_standing.py - ImprovedPDController

kp = [300, 250, 400, 500, 200,  # Left leg
      300, 250, 400, 500, 200,  # Right leg
      450,                       # Torso
      120, 120, 80, 80,          # Left arm
      120, 120, 80, 80]          # Right arm

kd = kp / 12.0

# Balance compensation
ankle_comp = -130 * pos_x - 22 * vel_x
hip_comp = -55 * pos_x - 9 * vel_x
torso_comp = -250 * pos_x - 30 * vel_x

# Standing pose
q_target = [
    0.0, 0.0, 0.05, 0.3, -0.15,  # Left leg
    0.0, 0.0, 0.05, 0.3, -0.15,  # Right leg
    0.0,                          # Torso
    0.2, 0.2, 0.0, -0.2,         # Left arm
    0.2, -0.2, 0.0, -0.2         # Right arm
]
```

---

## ğŸ“š References

### Official Resources

- [Unitree Robotics](https://www.unitree.com/)
- [MuJoCo Documentation](https://mujoco.readthedocs.io/)
- [Unitree SDK2 Python](https://github.com/unitreerobotics/unitree_sdk2_python)

### Related Projects

- [fan-ziqi/h1-mujoco-sim](https://github.com/fan-ziqi/h1-mujoco-sim) - H1 MuJoCo simulation
- [unitree_rl_gym](https://github.com/unitreerobotics/unitree_rl_gym) - Unitree RL training environment

### Academic References

- **ZMP Control**: VukobratoviÄ‡ & Borovac (2004) - "Zero-Moment Point"
- **Gait Planning**: Kajita et al. (2003) - "Biped Walking Pattern Generation"
- **PD Control**: Pratt et al. (2001) - "Virtual Model Control"

### â­ Known Working Unitree H1 Controllers

**Highly Recommended - Use these as reference!**

1. **Unitree Official**
   - [unitree_rl_gym](https://github.com/unitreerobotics/unitree_rl_gym) - Official RL training framework
   - [unitree_sdk2_python](https://github.com/unitreerobotics/unitree_sdk2_python) - Official SDK with working examples
   - **Status**: âœ… Production ready, hardware-tested

2. **RobotLoco** â­â­â­
   - [robot_loco](https://github.com/zitongbai/robot_loco) - Locomotion controller for H1
   - Includes: MPC + WBC implementation that actually works!
   - **Status**: âœ… Well-documented, simulation + real robot

3. **H1_MuJoCo_Controller**
   - [h1-mujoco-sim](https://github.com/fan-ziqi/h1-mujoco-sim) - Direct MuJoCo simulation
   - Simple PD controller with gravity compensation
   - **Status**: âœ… Stable standing and walking

4. **LeRobot (Hugging Face)**
   - [lerobot](https://github.com/huggingface/lerobot) - Includes H1 teleoperation
   - Uses Isaac Sim + MuJoCo
   - **Status**: âœ… Active development, well-maintained

5. **Whole-Body MPC**
   - [Crocoddyl + H1](https://github.com/machines-in-motion/mim_robots) - MPC-based controller
   - Research-grade optimal control
   - **Status**: âœ… Academic reference

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- **Unitree Robotics** - H1 humanoid robot model
- **MuJoCo Team** - Excellent physics engine
- **Pinocchio** - Efficient dynamics library
- **Groq** - Fast LLM inference
- **Open-source community** - Proven H1 control implementations ([unitree_rl_gym](https://github.com/unitreerobotics/unitree_rl_gym))
